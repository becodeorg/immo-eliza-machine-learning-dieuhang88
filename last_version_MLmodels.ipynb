{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b45335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4343bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Load Dataset\n",
    "df = pd.read_csv(\"../data/Kangaroo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c06c0",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "\n",
    "- EPC Score: remove the irrelevant symbol and mapping of EPC score (A -> G) into the mean of EPC classification (kWh/m2.year) by region (Wallonia / Flanders / Brussels). \n",
    "        df[epcScore] (object)  -> df[epc_enum] (float64)\n",
    "- Drops some columns missing 100% data and unneccessary \n",
    "- Drop missing values of these important features: 'price','habitableSurface','bedroomCount','bathroomCount','epcScore'\n",
    "- \n",
    "Removing Aberrant Values (Outlier Removal): based on the max of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9864e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping epcScore for each province\n",
    "def map_epc_score(df):\n",
    "    epc_invalid = ['C_A', 'F_C', 'G_C', 'D_C', 'F_D', 'E_C', 'G_E', 'E_D', 'C_B', 'X', 'G_F']\n",
    "    df = df[~df['epcScore'].isin(epc_invalid)].copy()\n",
    "\n",
    "    wallonia_provinces = ['Li√®ge', 'Walloon Brabant', 'Namur', 'Hainaut', 'Luxembourg']\n",
    "    flanders_provinces = ['Antwerp', 'Flemish Brabant', 'East Flanders', 'West Flanders', 'Limburg']\n",
    "\n",
    "    epc_maps = {\n",
    "        \"Wallonia\": {'A++': 0, 'A+': 30, 'A': 65, 'B': 125, 'C': 200, 'D': 300, 'E': 375, 'F': 450, 'G': 510},\n",
    "        \"Flanders\": {'A++': 0, 'A+': 0, 'A': 50, 'B': 150, 'C': 250, 'D': 350, 'E': 450, 'F': 500, 'G': 510},\n",
    "        \"Brussels\": {'A++': 0, 'A+': 0, 'A': 45, 'B': 75, 'C': 125, 'D': 175, 'E': 250, 'F': 300, 'G': 350}\n",
    "    }\n",
    "\n",
    "    def map_score(row):\n",
    "        if row['province'] in wallonia_provinces:\n",
    "            return epc_maps['Wallonia'].get(row['epcScore'], None)\n",
    "        elif row['province'] in flanders_provinces:\n",
    "            return epc_maps['Flanders'].get(row['epcScore'], None)\n",
    "        elif row['province'] == 'Brussels':\n",
    "            return epc_maps['Brussels'].get(row['epcScore'], None)\n",
    "        return None\n",
    "\n",
    "    df.loc[:, 'epc_enum'] = df.apply(map_score, axis=1)\n",
    "    return df\n",
    "\n",
    "# Cleaning Function\n",
    "def cleaning(df):\n",
    "    # Drop columns missing 100 % values and unnecessary columns\n",
    "    df = df.drop(columns=[\"Unnamed: 0\", \"url\", \"id\", \"monthlyCost\", \"accessibleDisabledPeople\", \"hasBalcony\"])\n",
    "    \n",
    "    # Drop columns not important\n",
    "    drop_cols = [\n",
    "        'roomCount', 'diningRoomSurface', 'streetFacadeWidth', 'kitchenSurface', 'hasBasement', 'hasArmoredDoor',\n",
    "        'floorCount', 'hasDiningRoom', 'hasDressingRoom', 'gardenSurface', 'terraceSurface', 'livingRoomSurface',\n",
    "        'gardenOrientation', 'heatingType', 'kitchenType', 'terraceOrientation'\n",
    "    ]\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    \n",
    "    # Drop rows with missing target or essential features\n",
    "    df = df.dropna(axis=0, subset=['price','habitableSurface','bedroomCount','bathroomCount','epcScore'])\n",
    "\n",
    "    # Convert boolean columns to binary (0/1)\n",
    "    binary_cols = [\n",
    "        'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 'hasAirConditioning', 'hasVisiophone', 'hasOffice',\n",
    "        'hasSwimmingPool', 'hasFireplace', 'hasAttic', 'parkingCountIndoor', 'parkingCountOutdoor'\n",
    "    ]\n",
    "    for col in binary_cols:\n",
    "        df[col] = df[col].map({True: 1, False: 0, 'True': 1, 'False': 0}).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "    # For facadeCount, rename and fill missing values based on subtype and remove rows with more than 4 facades\n",
    "    df = df[df['facedeCount'] <= 4]  # Remove rows with more than 4 facades\n",
    "    df['facadeCount'] = df['facedeCount']\n",
    "    df = df.drop(columns='facedeCount')\n",
    "\n",
    "    apartment_subtypes = ['APARTMENT', 'FLAT_STUDIO', 'GROUND_FLOOR', 'PENTHOUSE', 'APARTMENT_BLOCK']\n",
    "    df.loc[df['subtype'].isin(apartment_subtypes), 'facadeCount'] = df.loc[df['subtype'].isin(apartment_subtypes), 'facadeCount'].fillna(1)\n",
    "\n",
    "    house_subtypes = ['HOUSE', 'VILLA', 'DUPLEX', 'TOWN_HOUSE', 'MANSION']\n",
    "    df.loc[df['subtype'].isin(house_subtypes), 'facadeCount'] = df.loc[df['subtype'].isin(house_subtypes), 'facadeCount'].fillna(3)\n",
    "\n",
    "    larger_house_subtypes = ['EXCEPTIONAL_PROPERTY', 'BUNGALOW', 'COUNTRY_COTTAGE', 'TRIPLEX', 'CHALET', 'CASTLE', 'MANOR_HOUSE']\n",
    "    df.loc[df['subtype'].isin(larger_house_subtypes), 'facadeCount'] = df.loc[df['subtype'].isin(larger_house_subtypes), 'facadeCount'].fillna(4)\n",
    "\n",
    "    other_subtypes = ['MIXED_USE_BUILDING', 'SERVICE_FLAT', 'KOT', 'FARMHOUSE', 'LOFT', 'OTHER_PROPERTY']\n",
    "    df.loc[df['subtype'].isin(other_subtypes), 'facadeCount'] = df.loc[df['subtype'].isin(other_subtypes), 'facadeCount'].fillna(2)\n",
    "\n",
    "    condition_mapping = {\n",
    "        'AS_NEW': 0, 'JUST_RENOVATED': 1, 'GOOD': 2,\n",
    "        'TO_RENOVATE': 3, 'TO_RESTORE': 4, 'TO_BE_DONE_UP': 5\n",
    "    }\n",
    "    df['buildingCondition_mapping'] = df['buildingCondition'].map(condition_mapping)\n",
    "\n",
    "    flood_mapping = {\n",
    "        \"NON_FLOOD_ZONE\": 0,\n",
    "        \"POSSIBLE_N_CIRCUMSCRIBED_WATERSIDE_ZONE\": 1,\n",
    "        \"CIRCUMSCRIBED_WATERSIDE_ZONE\": 2,\n",
    "        \"POSSIBLE_N_CIRCUMSCRIBED_FLOOD_ZONE\": 3,\n",
    "        \"POSSIBLE_FLOOD_ZONE\": 4,\n",
    "        \"CIRCUMSCRIBED_FLOOD_ZONE\": 5,\n",
    "        \"RECOGNIZED_FLOOD_ZONE\": 6,\n",
    "        \"RECOGNIZED_N_CIRCUMSCRIBED_WATERSIDE_FLOOD_ZONE\": 7,\n",
    "        \"RECOGNIZED_N_CIRCUMSCRIBED_FLOOD_ZONE\": 8\n",
    "    }\n",
    "    df['floodZoneType_mapping'] = df['floodZoneType'].map(flood_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "# # Add postcode mean price feature\n",
    "# def add_postcode_price_mean(df):\n",
    "#     postcode_mean = df.groupby('postCode')['price'].mean().to_dict()\n",
    "#     df['postcode_price_mean'] = df['postCode'].map(postcode_mean)\n",
    "#     return df\n",
    "\n",
    "# ======= ADD price_per_m2 and postcode average price per m2 =======\n",
    "def add_postcode_price_mean(df):\n",
    "    # Calculate price per m2\n",
    "    df['price_per_m2'] = df['price'] / df['habitableSurface']\n",
    "    \n",
    "    # Compute average price_per_m2 per postcode\n",
    "    postcode_price = df.groupby('postCode')['price_per_m2'].mean().reset_index()\n",
    "    postcode_price.rename(columns={'price_per_m2': 'postcode_avg_price_per_m2'}, inplace=True)\n",
    "    \n",
    "    # Merge the average back to original df\n",
    "    df = df.merge(postcode_price, on='postCode', how='left')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f08d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all preprocessing\n",
    "df = map_epc_score(df)\n",
    "df = cleaning(df)\n",
    "df = add_postcode_price_mean(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67eab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature limits for manual outlier removal ---\n",
    "feature_limits = {\n",
    "    'price': (100000, 800000),\n",
    "    'habitableSurface': (20, 500),\n",
    "    'bathroomCount': (1, 10),\n",
    "    'bedroomCount': (1, 10),\n",
    "    #'landSurface': (0, 10000),  # Only for houses\n",
    "}\n",
    "\n",
    "def apply_feature_limits(df, limits):\n",
    "    df_filtered = df.copy()\n",
    "    for feature, (min_val, max_val) in limits.items():\n",
    "        df_filtered = df_filtered[(df_filtered[feature] >= min_val) & (df_filtered[feature] <= max_val)]\n",
    "    return df_filtered\n",
    "\n",
    "# --- Apply limits to remove outliers ---\n",
    "df = apply_feature_limits(df, feature_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a7fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "# Ensure boolean features are binary\n",
    "boolean_features = [\n",
    "    'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 'hasAirConditioning', \n",
    "    'hasVisiophone', 'hasOffice', 'hasSwimmingPool', 'hasFireplace', 'hasAttic', \n",
    "    'parkingCountIndoor', 'parkingCountOutdoor'\n",
    "]\n",
    "df[boolean_features] = df[boolean_features].astype(int)\n",
    "\n",
    "# Define feature groups\n",
    "numerical_features = [\n",
    "    'habitableSurface', 'bedroomCount', 'bathroomCount', 'facadeCount',\n",
    "    'landSurface', 'buildingConstructionYear', 'epc_enum', 'floodZoneType_mapping', 'buildingCondition_mapping', \n",
    "    'postcode_avg_price_per_m2'\n",
    "]\n",
    "categorical_features = [\n",
    "    'type', 'subtype', 'province'\n",
    "]\n",
    "# Define all features and target\n",
    "all_features = numerical_features + categorical_features + boolean_features\n",
    "X = df[all_features]\n",
    "y = df['price']\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features),\n",
    "    ('bool', 'passthrough', boolean_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  MAE  = 62644.74\n",
      "  RMSE = 84893.49\n",
      "  R¬≤   = 0.7094\n",
      "----------------------------------------\n",
      "Random Forest:\n",
      "  MAE  = 49413.71\n",
      "  RMSE = 69878.03\n",
      "  R¬≤   = 0.8031\n",
      "----------------------------------------\n",
      "Gradient Boosting:\n",
      "  MAE  = 53759.18\n",
      "  RMSE = 73624.76\n",
      "  R¬≤   = 0.7814\n",
      "----------------------------------------\n",
      "XGBoost:\n",
      "  MAE  = 49603.09\n",
      "  RMSE = 69209.85\n",
      "  R¬≤   = 0.8068\n",
      "----------------------------------------\n",
      "Neural Network (MLP):\n",
      "  MAE  = 61490.90\n",
      "  RMSE = 83569.54\n",
      "  R¬≤   = 0.7184\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Models to compare\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'Neural Network (MLP)': MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  MAE  = {mae:.2f}\")\n",
    "    print(f\"  RMSE = {rmse:.2f}\")\n",
    "    print(f\"  R¬≤   = {r2:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007099f9",
   "metadata": {},
   "source": [
    "### summary results of some tests for preprocessing with the accuracy of different models\n",
    "1. # for the same features: \n",
    "all_features = numerical_features + categorical_features + boolean_features\n",
    "boolean_features = [\n",
    "    'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 'hasAirConditioning', \n",
    "    'hasVisiophone', 'hasOffice', 'hasSwimmingPool', 'hasFireplace', 'hasAttic', \n",
    "    'parkingCountIndoor', 'parkingCountOutdoor'\n",
    "]\n",
    "df[boolean_features] = df[boolean_features].astype(int)\n",
    "\n",
    "# Define feature groups\n",
    "numerical_features = [\n",
    "    'habitableSurface', 'bedroomCount', 'bathroomCount', 'facadeCount',\n",
    "    'landSurface', 'buildingConstructionYear', 'epc_enum', 'floodZoneType_mapping', 'buildingCondition_mapping', \n",
    "    'postcode_avg_price_per_m2'\n",
    "]\n",
    "categorical_features = [\n",
    "    'type', 'subtype', 'province'\n",
    "]\n",
    "\n",
    "\n",
    "# 1.1. using median and most frequent for num (scaler, StandardScaler) and cat features (encoder, OneHotEncoder(handle_unknown='ignore') )\n",
    "### Result: \n",
    "Linear Regression - Mean Absolute Error: 181352.09\n",
    "Random Forest - Mean Absolute Error: 90434.27\n",
    "Gradient Boosting - Mean Absolute Error: 104593.16\n",
    "XGBoost - Mean Absolute Error: 94510.87\n",
    "Neural Network (MLP) - Mean Absolute Error: 183473.89\n",
    "\n",
    "# 1.2. Add outlier remove function: def apply_feature_limits(df, limits) \n",
    "feature_limits = {\n",
    "    'price': (100000, 1000000),\n",
    "    'habitableSurface': (20, 500),\n",
    "    'bathroomCount': (1, 10),\n",
    "    'bedroomCount': (1, 10),\n",
    "    'landSurface': (10, 10000),\n",
    "\n",
    "Linear Regression - Mean Absolute Error: 67607.64\n",
    "Random Forest - Mean Absolute Error: 59041.74\n",
    "Gradient Boosting - Mean Absolute Error: 55645.81\n",
    "XGBoost - Mean Absolute Error: 53134.02\n",
    "Neural Network (MLP) - Mean Absolute Error: 69159.10\n",
    "\n",
    "# add apartment\n",
    " feature_limits = {\n",
    "    'price': (100000, 800000),\n",
    "    'habitableSurface': (20, 500),\n",
    "    'bathroomCount': (1, 10),\n",
    "    'bedroomCount': (1, 10),\n",
    "\n",
    "Linear Regression - Mean Absolute Error: 62644.74\n",
    "Random Forest - Mean Absolute Error: 49413.71\n",
    "Gradient Boosting - Mean Absolute Error:  53759.18\n",
    "XGBoost - Mean Absolute Error:  49603.09\n",
    "Neural Network (MLP) - Mean Absolute Error: 61490.90\n",
    "\n",
    "# the range of feature_limits is small, the better accuracy \n",
    "feature_limits = {\n",
    "    'price': (100000, 800000),\n",
    "    'habitableSurface': (20, 800),\n",
    "    'bathroomCount': (1, 10),\n",
    "    'bedroomCount': (1, 10),\n",
    "    'landSurface': (10, 10000),\n",
    "}\n",
    "Linear Regression - Mean Absolute Error: 61062.43\n",
    "Random Forest - Mean Absolute Error: 54766.16\n",
    "Gradient Boosting - Mean Absolute Error: 56571.47\n",
    "XGBoost - Mean Absolute Error: 54081.98\n",
    "Neural Network (MLP) - Mean Absolute Error: 69936.27\n",
    "\n",
    "\n",
    "feature_limits = {\n",
    "    'price': (100000, 800000),\n",
    "    'habitableSurface': (20, 500),\n",
    "    'bathroomCount': (1, 10),\n",
    "    'bedroomCount': (1, 10),\n",
    "    'landSurface': (10, 10000),\n",
    "}\n",
    "Linear Regression - Mean Absolute Error: 60233.06\n",
    "Random Forest - Mean Absolute Error: 53438.65\n",
    "Gradient Boosting - Mean Absolute Error: 52555.35\n",
    "XGBoost - Mean Absolute Error: 54081.98\n",
    "Neural Network (MLP) - Mean Absolute Error: 68686.37\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6255dd7",
   "metadata": {},
   "source": [
    "### Improve the accuracy \n",
    "1. Train different models on subsets (for ex., separate models for houses vs apartments) and Use ensemble methods like stacking --> little bit better \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "952ab045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model results for Houses:\n",
      "Houses       | Linear Regression  | MAE =   63367.09 | RMSE =   86409.43 | R¬≤ = 0.7083\n",
      "Houses       | Random Forest      | MAE =   55108.82 | RMSE =   76377.77 | R¬≤ = 0.7721\n",
      "Houses       | Gradient Boosting  | MAE =   58088.23 | RMSE =   78423.54 | R¬≤ = 0.7597\n",
      "Houses       | XGBoost            | MAE =   54925.60 | RMSE =   75176.35 | R¬≤ = 0.7792\n",
      "\n",
      "Model results for Apartments:\n",
      "Apartments   | Linear Regression  | MAE =   52682.06 | RMSE =   74412.28 | R¬≤ = 0.7129\n",
      "Apartments   | Random Forest      | MAE =   42083.38 | RMSE =   62258.99 | R¬≤ = 0.7990\n",
      "Apartments   | Gradient Boosting  | MAE =   45298.70 | RMSE =   63918.68 | R¬≤ = 0.7881\n",
      "Apartments   | XGBoost            | MAE =   42718.52 | RMSE =   62000.11 | R¬≤ = 0.8007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m train_and_evaluate(df_houses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m train_and_evaluate(df_apartments, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApartments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m stacking_example(df_houses, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouses\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m stacking_example(df_apartments, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApartments\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[228], line 74\u001b[0m, in \u001b[0;36mstacking_example\u001b[1;34m(df_subset, subset_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m stacking_model \u001b[38;5;241m=\u001b[39m StackingRegressor(\n\u001b[0;32m     65\u001b[0m     estimators\u001b[38;5;241m=\u001b[39mestimators,\n\u001b[0;32m     66\u001b[0m     final_estimator\u001b[38;5;241m=\u001b[39mLinearRegression()\n\u001b[0;32m     67\u001b[0m )\n\u001b[0;32m     69\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     70\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     71\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstacking\u001b[39m\u001b[38;5;124m'\u001b[39m, stacking_model)\n\u001b[0;32m     72\u001b[0m ])\n\u001b[1;32m---> 74\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     75\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     77\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:958\u001b[0m, in \u001b[0;36mStackingRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the estimators.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m \n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;124;03m    Returns a fitted instance.\u001b[39;00m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    957\u001b[0m y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py:252\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m         cv\u001b[38;5;241m.\u001b[39mrandom_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState()\n\u001b[0;32m    249\u001b[0m     fit_params \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_weight} \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     )\n\u001b[1;32m--> 252\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    253\u001b[0m         delayed(cross_val_predict)(\n\u001b[0;32m    254\u001b[0m             clone(est),\n\u001b[0;32m    255\u001b[0m             X,\n\u001b[0;32m    256\u001b[0m             y,\n\u001b[0;32m    257\u001b[0m             cv\u001b[38;5;241m=\u001b[39mdeepcopy(cv),\n\u001b[0;32m    258\u001b[0m             method\u001b[38;5;241m=\u001b[39mmeth,\n\u001b[0;32m    259\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    260\u001b[0m             fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    261\u001b[0m             verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    262\u001b[0m         )\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m est, meth \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# Only not None or not 'drop' estimators will be used in transform.\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Remove the None from the method as well.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    270\u001b[0m     meth\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (meth, est) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_method_, all_estimators)\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m est \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:986\u001b[0m, in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 986\u001b[0m predictions \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    987\u001b[0m     delayed(_fit_and_predict)(\n\u001b[0;32m    988\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m splits\n\u001b[0;32m    991\u001b[0m )\n\u001b[0;32m    993\u001b[0m inv_test_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(test_indices), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    994\u001b[0m inv_test_indices[test_indices] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(test_indices))\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:1068\u001b[0m, in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1068\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m   1069\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(estimator, method)\n\u001b[0;32m   1070\u001b[0m predictions \u001b[38;5;241m=\u001b[39m func(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mapply_async(batch, callback\u001b[38;5;241m=\u001b[39mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[38;5;241m=\u001b[39mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "boolean_features = [\n",
    "    'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 'hasAirConditioning', \n",
    "    'hasVisiophone', 'hasOffice', 'hasSwimmingPool', 'hasFireplace', 'hasAttic', \n",
    "    'parkingCountIndoor', 'parkingCountOutdoor'\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'habitableSurface', 'bedroomCount', 'bathroomCount', 'facadeCount',\n",
    "    'landSurface', 'buildingConstructionYear', 'epc_enum', 'floodZoneType_mapping', 'buildingCondition_mapping', \n",
    "    'postcode_avg_price_per_m2'\n",
    "]\n",
    "\n",
    "categorical_features = ['type', 'subtype', 'province']\n",
    "\n",
    "all_features = numerical_features + categorical_features + boolean_features\n",
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42, eval_metric='rmse')\n",
    "}\n",
    "\n",
    "def train_and_evaluate(df_subset, subset_name):\n",
    "    if df_subset.empty:\n",
    "        print(f\"No data available for {subset_name}. Skipping...\")\n",
    "        return\n",
    "    \n",
    "    X = df_subset[all_features]\n",
    "    y = df_subset['price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"\\nModel results for {subset_name}:\")\n",
    "    for model_name, model in models.items():\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"{subset_name:<12} | {model_name:<18} | MAE = {mae:10.2f} | RMSE = {rmse:10.2f} | R¬≤ = {r2:6.4f}\")\n",
    "\n",
    "# --- Stacking model ---\n",
    "def stacking_example(df_subset, subset_name):\n",
    "    if df_subset.empty:\n",
    "        print(f\"No data available for {subset_name}. Skipping stacking...\")\n",
    "        return\n",
    "\n",
    "    X = df_subset[all_features]\n",
    "    y = df_subset['price']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    estimators = [\n",
    "        ('rf', RandomForestRegressor(random_state=42)),\n",
    "        ('gb', GradientBoostingRegressor(random_state=42)),\n",
    "        ('xgb', XGBRegressor(random_state=42, eval_metric='rmse'))\n",
    "    ]\n",
    "\n",
    "    stacking_model = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=LinearRegression()\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('stacking', stacking_model)\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"{subset_name:<12} | Stacking Model      | MAE = {mae:10.2f} | RMSE = {rmse:10.2f} | R¬≤ = {r2:6.4f}\")\n",
    "\n",
    "# --- Apply to subsets ---\n",
    "df_houses = df[df['type'].str.strip().str.lower() == 'house']\n",
    "df_apartments = df[df['type'].str.strip().str.lower() == 'apartment']\n",
    "\n",
    "train_and_evaluate(df_houses, \"Houses\")\n",
    "train_and_evaluate(df_apartments, \"Apartments\")\n",
    "\n",
    "stacking_example(df_houses, \"Houses\")\n",
    "stacking_example(df_apartments, \"Apartments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe378fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression |     | MAE =   62609.71 | RMSE =   84868.62 | R¬≤ = 0.7095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.333e+13, tolerance: 6.555e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression |     | MAE =   62614.18 | RMSE =   84874.11 | R¬≤ = 0.7095\n",
      "Random Forest |     | MAE =   49538.75 | RMSE =   69990.77 | R¬≤ = 0.8024\n",
      "XGBoost      |     | MAE =   47941.70 | RMSE =   67010.05 | R¬≤ = 0.8189\n"
     ]
    }
   ],
   "source": [
    "# Updated models with enhanced hyperparameters for RF and XGBoost\n",
    "models = {\n",
    "    'Ridge Regression': Ridge(alpha=1.0, random_state=42), \n",
    "    'Lasso Regression': Lasso(alpha=0.1, random_state=42, max_iter=10000), \n",
    "    'Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200,        # more trees\n",
    "        max_depth=None,            # control tree depth\n",
    "        min_samples_split=2,     # min samples to split\n",
    "        min_samples_leaf=2,      # min samples per leaf\n",
    "        max_features='auto',     # number of features to consider at each split\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,      # smaller learning rate\n",
    "        subsample=0.8,           # row sampling\n",
    "        colsample_bytree=0.8,    # feature sampling\n",
    "        random_state=42,\n",
    "        eval_metric='rmse'\n",
    "    )\n",
    "\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name:<12} |     | MAE = {mae:10.2f} | RMSE = {rmse:10.2f} | R¬≤ = {r2:6.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b02ac",
   "metadata": {},
   "source": [
    "## result for Random Forest |     | MAE =   52373.80 | RMSE =   72401.40 | R¬≤ = 0.7886\n",
    "Random Forest': RandomForestRegressor(\n",
    "        n_estimators=200,        # more trees\n",
    "        max_depth=20,            # control tree depth\n",
    "        min_samples_split=5,     # min samples to split\n",
    "        min_samples_leaf=2,      # min samples per leaf\n",
    "        max_features='sqrt',     # number of features to consider at each split\n",
    "        random_state=42\n",
    "    ),\n",
    "Lasso Regression |     | MAE =   62614.18 | RMSE =   84874.11 | R¬≤ = 0.7095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "38c007e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Random Forest...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dieu\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest params: {'model__max_depth': 28, 'model__max_features': 'auto', 'model__min_samples_leaf': 1, 'model__min_samples_split': 3, 'model__n_estimators': 229}\n",
      "Best RF CV MAE: 51758.25\n",
      "Tuning XGBoost...\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best XGBoost params: {'model__colsample_bytree': 0.7710164073434198, 'model__gamma': 0.12709563372047594, 'model__learning_rate': 0.042367428097991336, 'model__max_depth': 9, 'model__n_estimators': 340, 'model__subsample': 0.7257423924305306}\n",
      "Best XGB CV MAE: 48914.40\n",
      "Random Forest | MAE = 49300.83 | RMSE = 69685.05 | R¬≤ = 0.8042\n",
      "XGBoost | MAE = 46852.28 | RMSE = 65754.21 | R¬≤ = 0.8256\n"
     ]
    }
   ],
   "source": [
    "# Parameter distributions for Random Forest\n",
    "rf_param_dist = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__max_depth': randint(5, 30),\n",
    "    'model__min_samples_split': randint(2, 10),\n",
    "    'model__min_samples_leaf': randint(1, 10),\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Parameter distributions for XGBoost\n",
    "xgb_param_dist = {\n",
    "    'model__n_estimators': randint(100, 500),\n",
    "    'model__max_depth': randint(3, 15),\n",
    "    'model__learning_rate': uniform(0.01, 0.3),\n",
    "    'model__subsample': uniform(0.6, 0.4),\n",
    "    'model__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'model__gamma': uniform(0, 5)\n",
    "}\n",
    "\n",
    "# Create pipelines\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(random_state=42, eval_metric='rmse'))\n",
    "])\n",
    "\n",
    "# RandomizedSearchCV for Random Forest\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# RandomizedSearchCV for XGBoost\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=50,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit Random Forest\n",
    "print(\"Tuning Random Forest...\")\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest params:\", rf_search.best_params_)\n",
    "print(f\"Best RF CV MAE: {-rf_search.best_score_:.2f}\")\n",
    "\n",
    "# Fit XGBoost\n",
    "print(\"Tuning XGBoost...\")\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost params:\", xgb_search.best_params_)\n",
    "print(f\"Best XGB CV MAE: {-xgb_search.best_score_:.2f}\")\n",
    "\n",
    "# Evaluate best models on test set\n",
    "best_rf_model = rf_search.best_estimator_\n",
    "best_xgb_model = xgb_search.best_estimator_\n",
    "\n",
    "for model_name, model in [('Random Forest', best_rf_model), ('XGBoost', best_xgb_model)]:\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} | MAE = {mae:.2f} | RMSE = {rmse:.2f} | R¬≤ = {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
